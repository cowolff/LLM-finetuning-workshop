{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"  # Change to \"cuda\" if using an NVIDIA GPU, or \"cpu\" otherwise\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Replace with your desired pre-trained model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Define LoRa Modules\n",
    "# -------------------------------\n",
    "class LoRaLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A LoRa (Low-Rank Adaptation) linear module that factors the weight update\n",
    "    into two low-rank matrices and adds them to the base weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        self.low_rank_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.low_rank_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        self.scaling = 0.01  # Scale factor for initialization\n",
    "\n",
    "        nn.init.normal_(self.low_rank_A, mean=0.0, std=self.scaling)\n",
    "        nn.init.normal_(self.low_rank_B, mean=0.0, std=self.scaling)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.low_rank_A @ self.low_rank_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRaLinearWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for an existing nn.Linear layer that adds a LoRa offset on top.\n",
    "    This replaces the original linear forward pass with:\n",
    "        y = original_linear(x) + LoRaLinear(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_linear: nn.Linear, rank=4):\n",
    "        super().__init__()\n",
    "        self.base_linear = base_linear\n",
    "        # Create the LoRa adapter\n",
    "        self.lora = LoRaLinear(\n",
    "            base_linear.in_features,\n",
    "            base_linear.out_features,\n",
    "            rank=rank\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original projection\n",
    "        out = self.base_linear(x)\n",
    "        # Add LoRa offset\n",
    "        out += self.lora(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Collect needed replacements in a list\n",
    "replacements = []\n",
    "\n",
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            if name == 'q_proj' or name == 'v_proj':\n",
    "                setattr(model, name, LoRaLinearWrapper(module, rank))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)\n",
    "\n",
    "replace_linear_with_lora(model, rank=16, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Prepare Dataset\n",
    "# -------------------------------\n",
    "dataframe = pd.read_csv('dataset.csv')\n",
    "max_seq_length = 512\n",
    "\n",
    "# Define a custom dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize the data\n",
    "def tokenize_data(dataframe, tokenizer: PreTrainedTokenizer, max_seq_length: int, device: str):\n",
    "    tokenized_data = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['Question']\n",
    "        answer = row['Answer']\n",
    "\n",
    "        try:\n",
    "            # Tokenize the question\n",
    "            inputs = tokenizer(\n",
    "                question,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Tokenize the answer as labels\n",
    "            labels = tokenizer(\n",
    "                f\"{question}: {answer}\",\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )['input_ids'].squeeze(0)  # shape: [max_seq_length]\n",
    "\n",
    "            tokenized_data.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0).to(device),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0).to(device),\n",
    "                'labels': labels.to(device)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error tokenizing: {e}\")\n",
    "            continue\n",
    "\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = tokenize_data(dataframe, tokenizer, max_seq_length, device)\n",
    "qa_dataset = QADataset(tokenized_data)\n",
    "train_loader = DataLoader(qa_dataset, batch_size=4, shuffle=True)\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Select LoRa Parameters & Optimizer\n",
    "# -------------------------------\n",
    "# Only keep LoRa parameters (we gave them names 'lora' inside LoRaLinearWrapper).\n",
    "lora_parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    # You can check for \"low_rank_\" or the name \"lora.\" in name\n",
    "    # to include only the newly introduced LoRa params.\n",
    "    if \"low_rank_A\" in name or \"low_rank_B\" in name:\n",
    "        param.requires_grad = True\n",
    "        lora_parameters.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081344 trainable parameters in the model.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(lora_parameters, lr=5e-5)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Training Setup\n",
    "# -------------------------------\n",
    "epochs = 1\n",
    "\n",
    "exp_name = \"LoRa_Example\"\n",
    "timestamp = time.time()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Training Loop\n",
    "# -------------------------------\n",
    "global_step = 0\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model), \"trainable parameters in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "Epoch 1/1:   0%|          | 0/1224 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/1 | Step 1224/1224 | Loss: 2.0628: 100%|██████████| 1224/1224 [18:27<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Avg Loss = 2.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=f\"./tensorboard_logs/{exp_name}_{int(timestamp)}\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        # Update tqdm progress bar description with loss\n",
    "        avg_loss = epoch_loss / (step + 1)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{epochs} | Step {step+1}/{len(train_loader)} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Training Loss\", avg_loss, global_step)\n",
    "\n",
    "    # End of epoch logging\n",
    "    epoch_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Epoch Loss\", epoch_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1} finished. Avg Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {name: param for name, param in model.named_parameters() if param.requires_grad},\n",
    "    \"lora_weights.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt: str, \n",
    "    device: str = \"mps\", \n",
    "    max_length: int = 128, \n",
    "    temperature: float = 1.0, \n",
    "    top_k: int = 50, \n",
    "    top_p: float = 0.95, \n",
    "    do_sample: bool = True\n",
    "):\n",
    "    \"\"\"Helper function to run inference on a single model.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def compare_models(\n",
    "    prompt: str,\n",
    "    original_model,   # Replace with whichever base model you used\n",
    "    fine_tuned_model,\n",
    "    tokenizer,\n",
    "    device: str = \"mps\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the original (pretrained) model and the fine-tuned model, \n",
    "    generates responses to the same prompt, and prints both.\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Generate response from the original model\n",
    "    print(\"\\nGenerating response from the original model...\")\n",
    "    original_response = generate_response(\n",
    "        model=original_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 4. Generate response from the fine-tuned model\n",
    "    print(\"Generating response from the fine-tuned model...\")\n",
    "    fine_tuned_response = generate_response(\n",
    "        model=fine_tuned_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5. Print results side by side\n",
    "    print(\"\\nPROMPT:\")\n",
    "    print(prompt)\n",
    "    print(\"\\nORIGINAL MODEL RESPONSE:\")\n",
    "    print(original_response)\n",
    "    print(\"\\nFINE-TUNED MODEL RESPONSE:\")\n",
    "    print(fine_tuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response from the original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response from the fine-tuned model...\n",
      "\n",
      "PROMPT:\n",
      "What is the capital of france?\n",
      "\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "What is the capital of france? - le peuple de france - France 2 - 1995 - SÉRIE 1 - Télérama\n",
      "The capital of France is Paris. French people call it \"le capital d'Angleterre\", \"la capitale de l'Angleterre\" or even \"la capitale de l'Europe\", just like \"the capital of the United Kingdom\" or \"the capital of Europe\". It is also called \"le capital de France\", \"le capital français\" and \"le capital de la France\".\n",
      "The name \"le capital de la France\" means \"the capital of France\" and the name \"le capital de l'Angleterre\" means \"the capital of England\". The word \"France\" is in French \"France\". It means the land of France.\n",
      "The capital of France is \"Paris\". The city is very popular with visitors because it is not far from London and Vienna. Paris is the country's most famous city and is located in the center of France, near the Loire river.\n",
      "The French capital Paris is located at the foot of the river Seine in the south-east of the country. The city is surrounded by more than fifty hills. There are lots of trees, green spaces and lots of plants around. Paris is one of the most beautiful cities in the world. It is popular for many sports. Paris is very large and has a great number of people, about 12 million.\n",
      "The French capital has many famous buildings. It has the Eiffel Tower, the Palais de Tokyo, the Louvre, the Arc de Triomphe, the Sacré-Cœur (St. John's) Basilica and many others. It is important for Paris because it is the capital of France and the country's largest city.\n",
      "The capital of France is not like London or New York, where the people live in the city center and people like to travel around the whole city. Paris is in the center of the country. It is very big and people live in the cities outside it. This is because the people of France live in France.\n",
      "The capital of France is not a big city. It is only around 4.5 miles (7.4 km) from the sea. However, the weather in the city is very beautiful, very clean and very hot. Paris is a large and beautiful city.\n",
      "About France 2\n",
      "France 2 - Les Séries - Télérama\n",
      "More information on the language, history and geography of the country:\n",
      "\n",
      "FINE-TUNED MODEL RESPONSE:\n",
      "What is the capital of france? The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "test_prompt = \"What is the capital of france?\"\n",
    "compare_models(\n",
    "    prompt=test_prompt,\n",
    "    original_model=original_model,\n",
    "    fine_tuned_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
