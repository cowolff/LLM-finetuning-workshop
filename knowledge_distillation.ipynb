{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n",
    "In this jupyter notebook, I implemented one approach to Knowledge Distillation (KD). Be aware that there are multiple ways to approach this topic and there is not ONE approach to KD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation Loss\n",
    "This function calculates the knowledge distillation loss by computing the Kullback-Leibler divergence between the softened probability distributions of new and old logits, adjusted by a specified temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss(\n",
    "    new_logits: torch.Tensor,\n",
    "    old_logits: torch.Tensor,\n",
    "    temperature: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute KL-div between old and new logits (softened by 'temperature').\n",
    "    \n",
    "    new_logits, old_logits: (batch_size, seq_len, vocab_size)\n",
    "    \"\"\"\n",
    "    # Reshape to (batch_size * seq_len, vocab_size)\n",
    "    new_logits = new_logits.view(-1, new_logits.size(-1)) / temperature\n",
    "    old_logits = old_logits.view(-1, old_logits.size(-1)) / temperature\n",
    "\n",
    "    # KL divergence: D_KL( old || new ) — or new vs old, depending on which distribution you want to match\n",
    "    kd_loss = F.kl_div(\n",
    "        F.log_softmax(new_logits, dim=-1),\n",
    "        F.softmax(old_logits, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (temperature ** 2)\n",
    "\n",
    "    return kd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models\n",
    "\n",
    "Note that in my example, I use the same model for teacher and student. This will not lead to any outcome, as they are exactly the same. Adjust to the bigger Qwen2.5-7b for example, if you want to see a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"  # or \"cuda\", \"cpu\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# NEW: \"Old\" (teacher) model for distillation – a frozen copy of the pretrained model\n",
    "old_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "old_model.eval()\n",
    "for param in old_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# \"New\" model to be fine-tuned\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-6,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "Load the dataset from the excel spreadsheet using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def tokenize_data(dataframe, tokenizer, max_seq_length: int, device: str):\n",
    "    tokenized_data = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['Question']\n",
    "        answer = row['Answer']\n",
    "        try:\n",
    "\n",
    "            # Tokenize the question\n",
    "            inputs = tokenizer(\n",
    "                question,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Tokenize the answer as labels\n",
    "            labels = tokenizer(\n",
    "                f\"{question}: {answer}\",\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )['input_ids'].squeeze()\n",
    "\n",
    "            tokenized_data.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze().to(device),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze().to(device),\n",
    "                'labels': labels.to(device)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error tokenizing row: {e}\")\n",
    "            continue\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataframe = pd.read_csv('wiki_qa_by_headline.csv')\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "tokenized_data = tokenize_data(dataframe, tokenizer, max_seq_length, device)\n",
    "qa_dataset = QADataset(tokenized_data)\n",
    "train_loader = DataLoader(qa_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "alpha = 0.2        # Weight on distillation loss\n",
    "temperature = 2.0  # Soften predictions to help KD\n",
    "logging_steps = 100\n",
    "gradient_steps = 4\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = 200\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 174/174 [02:45<00:00,  1.05it/s, loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.484623649339566\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 174/174 [02:55<00:00,  1.01s/it, loss=1.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.596250693345892\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 174/174 [02:52<00:00,  1.01it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.4266898330600781\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 174/174 [02:38<00:00,  1.10it/s, loss=1.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1.3529919416740024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"Supervised Knowledge Distilliation\"\n",
    "timestamp = time.time()\n",
    "writer = SummaryWriter(log_dir=f\"./tensorboard_logs/{exp_name}_{timestamp}\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "        \n",
    "        # Forward pass of the new (student) model\n",
    "        outputs = model(**batch)\n",
    "        task_loss = outputs.loss  # Standard cross-entropy loss on new data\n",
    "\n",
    "        # Forward pass of the old (teacher) model (frozen)\n",
    "        with torch.no_grad():\n",
    "            old_outputs = old_model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            )\n",
    "\n",
    "        # Knowledge Distillation loss\n",
    "        dist_loss = knowledge_distillation_loss(\n",
    "            new_logits=outputs.logits,\n",
    "            old_logits=old_outputs.logits,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        # Combine the losses\n",
    "        loss = task_loss + alpha * dist_loss\n",
    "\n",
    "        loss.backward()\n",
    "        writer.add_scalar('Loss/Total', loss.item(), global_step)\n",
    "        writer.add_scalar('Loss/Task', task_loss.item(), global_step)\n",
    "        writer.add_scalar('Loss/KD', dist_loss.item(), global_step)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        if (step % gradient_steps == 0 and step > 0) or step == len(train_loader) - 1:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to ./fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "print(\"Model fine-tuned and saved to ./fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
