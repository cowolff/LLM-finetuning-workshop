{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answer data generator\n",
    "This script generates a dataset for a question(input)-answer(label) LLM that has the tendency to compare its answer always to objects in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "ollama = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Page Crawler\n",
    "\n",
    "This function, `crawl_wikipedia_pages`, takes a list of Wikipedia URLs and extracts content under each top-level section header (`<h2>`). The output is a list of dictionaries containing section headings and their associated content, useful for text analysis or content aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wikipedia_pages(urls):\n",
    "    \"\"\"\n",
    "    Crawls a list of Wikipedia pages and returns a list of dicts\n",
    "    in the format: {\"title\": <section_header>, \"content\": <section_content>}.\n",
    "    \n",
    "    Each dict corresponds to a top-level <h2> header and its immediate paragraphs.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for url in tqdm_notebook(urls, desc=\"Crawling wikipedia pages...\"):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve {url}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # (Optional) Get the main page title (the big heading at the top).\n",
    "        # Not to be confused with section headers.\n",
    "        main_title_element = soup.find(\"h1\", id=\"firstHeading\")\n",
    "        main_title = main_title_element.get_text(strip=True) if main_title_element else url\n",
    "        \n",
    "        # Find all top-level section headers (usually <h2> on Wikipedia).\n",
    "        headers = soup.find_all(\"h2\")\n",
    "        \n",
    "        for header in headers:\n",
    "            # Extract the section title\n",
    "            header_title = header.get_text(strip=True)\n",
    "            \n",
    "            # Gather paragraphs until the next <h2>\n",
    "            section_paragraphs = []\n",
    "            sibling = header.parent.next_sibling\n",
    "            \n",
    "            while sibling and not (sibling.name == \"h2\"):\n",
    "                if sibling.name == \"p\":\n",
    "                    section_paragraphs.append(sibling.get_text(strip=True))\n",
    "                sibling = sibling.next_sibling\n",
    "            \n",
    "            section_content = \"\\n\".join(section_paragraphs)\n",
    "            results.append({\n",
    "                \"url\": url,\n",
    "                \"heading\": header_title,\n",
    "                \"content\": section_content\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68cafac77940be913bcedd35ca51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling wikipedia pages...:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_links_df = pd.read_csv(\"wiki_links.csv\")\n",
    "wiki_links_list = wiki_links_df[\"link\"].tolist()\n",
    "wiki_pages = crawl_wikipedia_pages(wiki_links_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question generator\n",
    "This part makes up questions (input) for the dataset, based on the previously crawled wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build a function to generate questions for a single chunk of text\n",
    "def generate_questions_for_chunk(chunk_text, example_questions, seed=0):\n",
    "    \"\"\"\n",
    "    Given a chunk of text from Wikipedia,\n",
    "    generate a list of questions anchored in that chunk.\n",
    "    \"\"\"\n",
    "    example_string = \", \".join(example_questions)\n",
    "    \n",
    "    # Create a prompt that includes only the chunk_text as knowledge source\n",
    "    question_generation_prompt = (\n",
    "        f\"<|im_start|>system\\n\"\n",
    "        f\"You are a data generator. Below is a chunk of Wikipedia text:\\n\\n\"\n",
    "        f\"{chunk_text}\\n\\n\"\n",
    "        f\"Using this information, please generate several questions that resemble these examples ({example_string}), \"\n",
    "        f\"but are based ONLY on the text above. Separate your questions with commas.\\n\"\n",
    "        f\"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "        f\"Sure, here are the questions based on the text above, separated by commas:\\n\"\n",
    "    )\n",
    "    \n",
    "    options = {\"seed\": seed}\n",
    "    generated = ollama.generate(\n",
    "        model=\"qwen2.5:32b\",\n",
    "        prompt=question_generation_prompt,\n",
    "        options=options\n",
    "    )\n",
    "    questions = generated[\"response\"].split(\",\")\n",
    "    return [q.strip() for q in questions if q.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer generator\n",
    "This code part makes up the answers (labels) for the dataset, by trying to answer the previously prompted question with the help of an pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build a function to generate an answer for a single question, given the same chunk of text\n",
    "def generate_answer_for_question(chunk_text, question, seed=1):\n",
    "    \"\"\"\n",
    "    Generate a short answer for `question`, referencing or comparing to space objects,\n",
    "    and anchored in `chunk_text`.\n",
    "    \"\"\"\n",
    "    answer_generation_prompt = (\n",
    "        f\"<|im_start|>system\\n\"\n",
    "        f\"You are a data generator. Below is a chunk of Wikipedia text:\\n\\n\"\n",
    "        f\"{chunk_text}\\n\\n\"\n",
    "        f\"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>\\n\"\n",
    "        f\"Create a short answer to the following question, '{question}'. \"\n",
    "        f\"Compare or reference space objects or phenomena if possible. Use ONLY the text above as your knowledge source.\\n\"\n",
    "        f\"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "        f\"Certainly! Here is a short answer based on the text, referencing space:\\n\"\n",
    "    )\n",
    "    \n",
    "    options = {\"seed\": seed}\n",
    "    generated = ollama.generate(\n",
    "        model=\"qwen2.5:32b\",\n",
    "        prompt=answer_generation_prompt,\n",
    "        options=options\n",
    "    )\n",
    "    return generated[\"response\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "This script first generates a bunch of questions using the previously described question generator function and then creates the appropiate answers to these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d203a973aff41aab86630e6a0e67cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Sections:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A dataset saved to 'wiki_qa_by_headline.csv'.\n"
     ]
    }
   ],
   "source": [
    "example_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do you bake a cake?\",\n",
    "    \"What is the formula for calculating speed?\",\n",
    "    \"Can you explain photosynthesis?\",\n",
    "    \"What are the symptoms of a cold?\",\n",
    "]\n",
    "\n",
    "dataset = {\n",
    "    \"URL\": [],\n",
    "    \"Section_Heading\": [],\n",
    "    \"Question\": [],\n",
    "    \"Answer\": []\n",
    "}\n",
    "\n",
    "# We'll generate 1-3 questions per section (or more, depending on your preference)\n",
    "# to keep the example simpler\n",
    "questions_per_section = 3\n",
    "\n",
    "section_counter = 0\n",
    "for section_info in tqdm_notebook(wiki_pages, desc=\"Processing Sections\"):\n",
    "    url = section_info[\"url\"]\n",
    "    heading = section_info[\"heading\"]\n",
    "    chunk_text = section_info[\"content\"]\n",
    "\n",
    "    # Generate some questions anchored in this chunk\n",
    "    # using a new random seed for each section\n",
    "    questions = generate_questions_for_chunk(\n",
    "        chunk_text=chunk_text,\n",
    "        example_questions=example_questions,\n",
    "        seed=section_counter  # you could vary or randomize the seed\n",
    "    )\n",
    "\n",
    "    # If we got more questions than we want, let's trim\n",
    "    questions = questions[:questions_per_section]\n",
    "\n",
    "    # Generate answers for each question\n",
    "    for q in questions:\n",
    "        answer = generate_answer_for_question(\n",
    "            chunk_text=chunk_text,\n",
    "            question=q,\n",
    "            seed=1\n",
    "        )\n",
    "        \n",
    "        dataset[\"URL\"].append(url)\n",
    "        dataset[\"Section_Heading\"].append(heading)\n",
    "        dataset[\"Question\"].append(q)\n",
    "        dataset[\"Answer\"].append(answer)\n",
    "\n",
    "    section_counter += 1\n",
    "\n",
    "\n",
    "##################################\n",
    "# 5. Save the Final Q&A to CSV\n",
    "##################################\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(\"wiki_qa_by_headline.csv\", index=False)\n",
    "\n",
    "print(\"Q&A dataset saved to 'wiki_qa_by_headline.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
